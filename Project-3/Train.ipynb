{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "import statistics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_validate,train_test_split,StratifiedKFold,KFold\n",
    "import pickle\n",
    "\n",
    "def DataPreProcessing(CGM_Data):\n",
    "    no_of_rows=CGM_Data.shape[0]\n",
    "    no_of_columns = CGM_Data.shape[1]\n",
    "    CGM_Data.dropna(axis=0, how='all', thresh=no_of_columns/4, subset=None, inplace=True)\n",
    "    CGM_Data.dropna(axis=1, how='all', thresh=no_of_rows/4, subset=None, inplace=True)\n",
    "    CGM_Data.interpolate(axis=0, method ='linear', limit_direction ='forward', inplace=True)\n",
    "    CGM_Data.bfill(axis=1,inplace=True)\n",
    "    return CGM_Data\n",
    "\n",
    "def ExtractFeatures(CGM_Data):\n",
    "    \n",
    "    Feature_Matrix = pd.DataFrame() \n",
    "\n",
    "    # Feature 1 - Fast Fourier Transform\n",
    "    FFT = pd.DataFrame()\n",
    "    def calculate_fft_vals(series):\n",
    "        FFT_abs = abs(fft(series))\n",
    "        FFT_abs.sort()\n",
    "        return np.flip(FFT_abs)[0:8]\n",
    "\n",
    "    FFT['FFT_vals'] = CGM_Data.apply(lambda series: calculate_fft_vals(series), axis=1)\n",
    "    FFT_Vals= pd.DataFrame(FFT.FFT_vals.tolist(), columns=['FFT1', 'FFT2', 'FFT3', 'FFT4', 'FFT5', 'FFT6', 'FFT7','FFT8'],index=FFT.FFT_vals.index)\n",
    "    Feature_Matrix = pd.concat([Feature_Matrix,FFT_Vals],axis=1)\n",
    "    \n",
    "    \n",
    "    # Feature 2 - Max of CGM Velocity \n",
    "    \n",
    "    Velocity_Data = pd.DataFrame()\n",
    "    win_size=6\n",
    "    total_vals=CGM_Data.shape[1]-win_size\n",
    "\n",
    "    for index in range(0, total_vals):\n",
    "        dv = CGM_Data.iloc[:, index + win_size] - CGM_Data.iloc[:, index]\n",
    "        Velocity_Data['vel'+str(index)] = dv\n",
    "\n",
    "    Feature_Matrix['Max CGM Vel']=Velocity_Data.max(axis = 1,skipna=True)\n",
    "    \n",
    "        \n",
    "    # Feature 3 - polyfit   \n",
    "    def calculate_polyfit(series,degree=3):\n",
    "        row_arr = np.array(series.index)\n",
    "        return np.polyfit(row_arr, series, degree)\n",
    "    \n",
    "    Polyfit_vals = CGM_Data.apply(calculate_polyfit,axis=1,result_type='expand')\n",
    "    Feature_Matrix = pd.concat([Feature_Matrix,Polyfit_vals],axis=1)\n",
    "     \n",
    " \n",
    "    return Feature_Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Kmeans_model(X_principal):\n",
    "    \n",
    "    print(\"----------------- K-means---------------\")\n",
    "      \n",
    "    clusterNum = 6\n",
    "    k_means = KMeans(init = \"k-means++\", n_clusters = clusterNum, n_init = 12)\n",
    "    k_means.fit(X_principal)\n",
    "    kmeans_labels = k_means.labels_\n",
    "    print(\"K-means labels\", kmeans_labels)\n",
    "    \n",
    "    sse = k_means.inertia_\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"SSE of K_means clustering is : \", sse)\n",
    "    print(\"-----------------------------------------\")\n",
    "    \n",
    "    # Categorize all the rows into clusters formed by K-means\n",
    "    KMeans_Clusters = []                                                           \n",
    "    for bin in range (0, 6):                               \n",
    "        new = []                  \n",
    "        for i in range (0,len(kmeans_labels)):\n",
    "            if(kmeans_labels[i]==bin):\n",
    "                new.append(i)     \n",
    "        KMeans_Clusters.append(new)\n",
    "        \n",
    "    # Match K-means labels with ground truth labels and update the K-means labels\n",
    "    def most_frequent(List): \n",
    "        return max(set(List), key = List.count)\n",
    "    \n",
    "    Updated_kmeans_labels = kmeans_labels.copy()\n",
    "    # Loop through each K-means cluster   \n",
    "    for c in range(0,6):\n",
    "        kmeans_cluster = KMeans_Clusters[c]\n",
    "        updated_label = 0 \n",
    "        true_labels = []\n",
    "        # Determine the ground truth label for the kmeans cluster based on majority\n",
    "        for i in range(0,len(kmeans_cluster)):\n",
    "            val = kmeans_cluster[i]\n",
    "            true_labels.append(Ground_Truth_list[val])\n",
    "        updated_label = most_frequent(true_labels)\n",
    "        # Update the kmeans labels\n",
    "        for i in range(0,len(kmeans_cluster)):\n",
    "            val = kmeans_cluster[i]\n",
    "            Updated_kmeans_labels[val] = updated_label\n",
    "        \n",
    "    print(\"Updated K-means labels\", Updated_kmeans_labels)\n",
    "    \n",
    "    knn=KNeighborsClassifier(algorithm='auto', leaf_size=10, n_neighbors=20, p=2, weights='uniform')\n",
    "    knn.fit(X_principal,Updated_kmeans_labels)\n",
    "\n",
    "    accuracy = accuracy_score(Updated_kmeans_labels,Ground_Truth_list)*100\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Accuracy of K-Means clustering \",accuracy)\n",
    "    print(\"-----------------------------------------\")\n",
    "    \n",
    "    handler = open(\"KMeans.model\",\"wb\")\n",
    "    knn = KNeighborsClassifier(n_neighbors=20)\n",
    "    knn.fit(X_principal,Updated_kmeans_labels)\n",
    "    pickle.dump(knn,handler)\n",
    "    handler.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DBSCAN_model(X_principal):\n",
    "    \n",
    "    print(\"----------------- DBSCAN---------------\")\n",
    "    db = DBSCAN(eps=0.157, min_samples=7)\n",
    "    db.fit(X_principal)\n",
    "    db_labels = db.labels_\n",
    "    print(\"DBSCAN labels\", db_labels)\n",
    "    \n",
    "    # Categorize all the rows into clusters formed by DBSCAN\n",
    "    DBSCAN_Clusters = []                                                           \n",
    "    for bin in range (-1, 6):                               \n",
    "        new = []                  \n",
    "        for i in range (0,len(db_labels)):\n",
    "            if(db_labels[i]==bin):\n",
    "                new.append(i)     \n",
    "        DBSCAN_Clusters.append(new)\n",
    "        \n",
    "    # Match DBSCAN labels with ground truth labels and update the DBSCAN labels\n",
    "    def most_frequent(List): \n",
    "        return max(set(List), key = List.count)\n",
    "    \n",
    "    Updated_dbscan_labels = db_labels.copy()\n",
    "    # Loop through each DBSCAN cluster   \n",
    "    for c in range(0,7):\n",
    "        db_cluster = DBSCAN_Clusters[c]\n",
    "        updated_label = 0 \n",
    "        true_labels = []\n",
    "        # Determine the ground truth label for the dbscan cluster based on majority\n",
    "        for i in range(0,len(db_cluster)):\n",
    "            val = db_cluster[i]\n",
    "            true_labels.append(Ground_Truth_list[val])\n",
    "        updated_label = most_frequent(true_labels)\n",
    "        # Update the dbscan labels\n",
    "        for i in range(0,len(db_cluster)):\n",
    "            val = db_cluster[i]\n",
    "            Updated_dbscan_labels[val] = updated_label    \n",
    "        \n",
    "    print(\"Updated DBSCAN labels\", Updated_dbscan_labels)\n",
    "    \n",
    "    knn=KNeighborsClassifier(algorithm='auto', leaf_size=10, n_neighbors=20, p=2, weights='uniform')\n",
    "    knn.fit(X_principal,Updated_dbscan_labels)\n",
    "\n",
    "    accuracy = accuracy_score(Updated_dbscan_labels,Ground_Truth_list)*100\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Accuracy of DBSCAN clustering \",accuracy)\n",
    "    print(\"-----------------------------------------\")\n",
    "    \n",
    "    handler = open(\"DBSCAN.model\",\"wb\")\n",
    "    knn = KNeighborsClassifier(n_neighbors=20)\n",
    "    knn.fit(X_principal,Updated_dbscan_labels)\n",
    "    pickle.dump(knn,handler)\n",
    "    handler.close()\n",
    "    \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth labels [4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 6, 3, 5, 4, 6, 3, 5, 4, 6, 3, 5, 4, 6, 1, 5, 2, 6, 1, 5, 2, 6, 1, 5, 2, 6, 1, 5, 3, 6, 4, 3, 2, 1, 3, 2, 1, 1, 5, 1, 2, 3, 2, 3, 2, 1, 5, 1, 2, 3, 2, 3, 2, 1, 5, 1, 2, 3, 2, 3, 2, 1, 5, 1, 3, 2, 3, 2, 1, 5, 1, 2, 3, 2, 3, 2, 1, 5, 1, 1, 1, 1, 1, 4, 1, 3, 4, 1, 4, 1, 3, 4, 1, 4, 1, 3, 4, 1, 1, 3, 4, 1, 4, 1, 3, 4, 1, 1, 1, 4, 4, 2, 1, 1, 1, 1, 1, 4, 4, 2, 1, 1, 1, 1, 4, 4, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 3, 4, 2, 2, 2, 4, 2, 3, 1, 3, 4, 2, 2, 2, 4, 2, 3, 1, 3, 4, 2, 2, 2, 4, 2, 3, 1, 3, 4, 2, 2, 2, 4, 2, 3, 1, 3, 4, 2, 2, 2, 1, 4, 4, 1, 4, 4, 4, 1, 4, 4, 4, 1, 4, 4, 4, 1, 4, 4, 2, 2, 1, 4, 4, 2, 1, 4, 4, 2, 2, 1, 4, 4, 2, 2, 1, 4, 4, 2, 2, 1, 4, 3, 1, 1, 4, 1, 4, 3, 1, 1]\n",
      "----------------- K-means---------------\n",
      "K-means labels [4 0 1 0 0 5 1 3 4 5 3 1 2 0 5 3 3 1 5 2 3 3 4 0 5 1 2 2 0 3 1 3 3 4 1 5 3\n",
      " 3 3 0 5 4 5 1 4 1 1 4 0 3 3 2 4 2 3 2 0 0 4 1 1 3 3 1 1 5 3 0 3 3 3 1 2 5\n",
      " 0 3 2 0 2 2 2 4 0 4 5 4 2 0 0 4 4 4 2 3 1 3 0 2 4 2 4 4 4 5 3 5 2 4 4 2 4\n",
      " 4 4 4 2 5 4 4 2 2 1 2 4 2 2 4 4 2 4 2 2 4 3 2 2 2 4 4 4 2 2 4 1 2 4 4 4 4\n",
      " 0 4 4 5 3 3 5 5 3 3 3 2 2 1 3 3 1 1 1 1 3 4 1 1 1 1 5 5 4 4 0 5 5 5 5 5 3\n",
      " 4 5 1 2 5 5 2 2 2 4 4 5 5 2 0 4 5 3 3 1 4 4 4 1 3 1 3 3 3 3 2 0 3 3 4 3 0\n",
      " 0 0 1 1 0 1 1 0 1 5 0 2 0 0 1 5 0 5 4 3 3 1 0]\n",
      "-----------------------------------------\n",
      "SSE of K_means clustering is :  18.59165233413163\n",
      "-----------------------------------------\n",
      "Updated K-means labels [1 2 1 2 2 4 1 4 1 4 4 1 1 2 4 4 4 1 4 1 4 4 1 2 4 1 1 1 2 4 1 4 4 1 1 4 4\n",
      " 4 4 2 4 1 4 1 1 1 1 1 2 4 4 1 1 1 4 1 2 2 1 1 1 4 4 1 1 4 4 2 4 4 4 1 1 4\n",
      " 2 4 1 2 1 1 1 1 2 1 4 1 1 2 2 1 1 1 1 4 1 4 2 1 1 1 1 1 1 4 4 4 1 1 1 1 1\n",
      " 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 2 1 1 4 4 4 4 4 4 4 4 1 1 1 4 4 1 1 1 1 4 1 1 1 1 1 4 4 1 1 2 4 4 4 4 4 4\n",
      " 1 4 1 1 4 4 1 1 1 1 1 4 4 1 2 1 4 4 4 1 1 1 1 1 4 1 4 4 4 4 1 2 4 4 1 4 2\n",
      " 2 2 1 1 2 1 1 2 1 4 2 1 2 2 1 4 2 4 1 4 4 1 2]\n",
      "-----------------------------------------\n",
      "Accuracy of K-Means clustering  30.612244897959183\n",
      "-----------------------------------------\n",
      "----------------- DBSCAN---------------\n",
      "DBSCAN labels [ 0  0 -1  0  0  1  2  2  0  1  2  2  0  0  1 -1 -1  2 -1  0  4  4  0  0\n",
      "  1 -1  5 -1 -1  2  3  4  2  0  3  1  2  4  2 -1  1  0 -1  3  0  2 -1  0\n",
      "  1  2 -1  0  0  0  2  0  0 -1  0  3  3  2  2  2  2  1 -1 -1  2  2  2  2\n",
      "  0  1  0  4  5 -1  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  2  3  2\n",
      "  0  0  0 -1  0  0  0  1 -1  1 -1  0  0  0  0  0  0  0  0 -1  0  0  0  0\n",
      "  2  5  0  5  0  0  0  0  0  0  5  0  4  0  0  0  0  0  0  0 -1  0  2  0\n",
      "  0  0  0  0 -1  0  0  1  2  2 -1  1  2  2  2  5 -1  2  2  2  2  2 -1  2\n",
      "  2  0  2  2  2  2  1  1  0  0  0 -1  1  1  1  1 -1  0 -1  2  0  1  1 -1\n",
      "  0  5  0  0  1  1  0 -1  0  1 -1 -1  2  0  0  0  2 -1 -1 -1  3  2  2  0\n",
      "  0  4  2  0 -1  0 -1  0  2  2  0  2  3 -1  2  1  0  0  0  0 -1  1 -1  1\n",
      "  0  2  2  2  0]\n",
      "Updated DBSCAN labels [1 1 4 1 1 4 2 2 1 4 2 2 1 1 4 4 4 2 4 1 4 4 1 1 4 4 1 4 4 2 1 4 2 1 1 4 2\n",
      " 4 2 4 4 1 4 1 1 2 4 1 4 2 4 1 1 1 2 1 1 4 1 1 1 2 2 2 2 4 4 4 2 2 2 2 1 4\n",
      " 1 4 1 4 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 2 1 2 1 1 1 4 1 1 1 4 4 4 4 1 1 1 1\n",
      " 1 1 1 1 4 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 4 1 2 1 1 1 1 1\n",
      " 4 1 1 4 2 2 4 4 2 2 2 1 4 2 2 2 2 2 4 2 2 1 2 2 2 2 4 4 1 1 1 4 4 4 4 4 4\n",
      " 1 4 2 1 4 4 4 1 1 1 1 4 4 1 4 1 4 4 4 2 1 1 1 2 4 4 4 1 2 2 1 1 4 2 1 4 1\n",
      " 4 1 2 2 1 2 1 4 2 4 1 1 1 1 4 4 4 4 1 2 2 2 1]\n",
      "-----------------------------------------\n",
      "Accuracy of DBSCAN clustering  33.06122448979592\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # Read Meal Data\n",
    "    column_names = [i for i in range(0,31)]\n",
    "    meal_data_1 = pd.read_csv(\"mealData1.csv\",names=column_names)\n",
    "    meal_data_2 = pd.read_csv(\"mealData2.csv\",names=column_names)\n",
    "    meal_data_3 = pd.read_csv(\"mealData3.csv\",names=column_names)\n",
    "    meal_data_4 = pd.read_csv(\"mealData4.csv\",names=column_names)\n",
    "    meal_data_5 = pd.read_csv(\"mealData5.csv\",names=column_names)\n",
    "    CGM_Meal_Data = pd.concat([meal_data_1,meal_data_2,meal_data_3,meal_data_4,meal_data_5],axis=0,ignore_index=True)\n",
    "    \n",
    "    # Data Pre-Processing\n",
    "    CGM_Meal_Data = DataPreProcessing(CGM_Meal_Data)\n",
    "    \n",
    "    # Read meal amounts \n",
    "    meal_amt_1 = pd.read_csv(\"mealAmountData1.csv\",names= [\"Meal Amt\"],nrows=len(meal_data_1))\n",
    "    meal_amt_2 = pd.read_csv(\"mealAmountData2.csv\",names= [\"Meal Amt\"],nrows=len(meal_data_2))\n",
    "    meal_amt_3 = pd.read_csv(\"mealAmountData3.csv\",names= [\"Meal Amt\"],nrows=len(meal_data_3))\n",
    "    meal_amt_4 = pd.read_csv(\"mealAmountData4.csv\",names= [\"Meal Amt\"],nrows=len(meal_data_4))\n",
    "    meal_amt_5 = pd.read_csv(\"mealAmountData5.csv\",names= [\"Meal Amt\"],nrows=len(meal_data_5))\n",
    "    CGM_Meal_Amt = pd.concat([meal_amt_1,meal_amt_2,meal_amt_3,meal_amt_4,meal_amt_5],axis=0, ignore_index=True)\n",
    "    \n",
    "    # Assign ground truth labels to meal amounts\n",
    "    Meal_Amt_label = pd.DataFrame()\n",
    "    \n",
    "    def label_val(x):\n",
    "        if(x==0):\n",
    "            return int(x)+1\n",
    "        elif(x%20==0):\n",
    "            return int(x/20)+1\n",
    "        else:\n",
    "            return int(x/20)+2\n",
    "\n",
    "    Meal_Amt_label['BINS'] = CGM_Meal_Amt.apply(lambda row: label_val(row['Meal Amt']), axis=1)   \n",
    "\n",
    "    # Join Meal Data and Meal Amount\n",
    "    Meal_Data_and_Amt = CGM_Meal_Data.join(Meal_Amt_label)\n",
    "    Meal_Data_and_Amt = Meal_Data_and_Amt.reset_index(drop=True)\n",
    "    \n",
    "    # Categorize all the ground truth labels into clusters \n",
    "    Ground_Truth_list = Meal_Data_and_Amt[\"BINS\"].tolist()\n",
    "    \n",
    "    Ground_Truth_Clusters = []                                                           \n",
    "    for bin in range (0, 6):                               \n",
    "        new = []                  \n",
    "        for i in range (0,len(Ground_Truth_list)):\n",
    "            if(Ground_Truth_list[i]==bin):\n",
    "                new.append(i)     \n",
    "        Ground_Truth_Clusters.append(new)  \n",
    "    print(\"Ground Truth labels\", Ground_Truth_list)\n",
    "\n",
    "    #Extract Features\n",
    "    Feature_Matrix = ExtractFeatures(CGM_Meal_Data) \n",
    "    Feature_Matrix = Feature_Matrix.reset_index(drop=True)\n",
    "    \n",
    "    # Standardize feature matrix\n",
    "    Feature_Matrix_std = StandardScaler().fit_transform(Feature_Matrix)\n",
    "    \n",
    "    # Normalize the data so that the data follows a Gaussian distribution\n",
    "    Feature_Matrix_norm = normalize(Feature_Matrix_std)\n",
    "    Feature_Matrix_norm = pd.DataFrame(Feature_Matrix_norm)\n",
    "    \n",
    "    # Do PCA\n",
    "    pca=PCA(n_components=2)\n",
    "    X_principal = pca.fit_transform(Feature_Matrix_norm)\n",
    "    X_principal = pd.DataFrame(X_principal)\n",
    "    X_principal.columns = ['PCA1','PCA2']\n",
    "    \n",
    "    # Train KMeans model\n",
    "    train_Kmeans_model(X_principal)\n",
    "    \n",
    "    # Train DBSCAN model\n",
    "    train_DBSCAN_model(X_principal)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
